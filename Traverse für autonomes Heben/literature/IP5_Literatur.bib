
@article{kalaitzakis_fiducial_2021,
	title = {Fiducial Markers for Pose Estimation},
	volume = {101},
	issn = {1573-0409},
	url = {https://doi.org/10.1007/s10846-020-01307-9},
	doi = {10.1007/s10846-020-01307-9},
	abstract = {Robust localization is critical for the navigation and control of mobile robots. Global Navigation Satellite Systems ({GNSS}), Visual-Inertial Odometry ({VIO}), and Simultaneous Localization and Mapping ({SLAM}) offer different methods for achieving this goal. In some cases however, these methods may not be available or provide high enough accuracy. In such cases, these methods may be augmented or replaced with fiducial marker pose estimation. Fiducial markers can increase the accuracy and robustness of a localization system by providing an easily recognizable feature with embedded fault detection. This paper presents an overview of fiducial markers developed in the recent years and an experimental comparison of the four markers ({ARTag}, {AprilTag}, {ArUco}, and {STag}) that represent the state-of-the-art and most widely used packages. These markers are evaluated on their accuracy, detection rate and computational cost in several scenarios that include simulated noise from shadows and motion blur. Different marker configurations, including single markers, planar and non-planar bundles and multi-sized marker bundles are also considered in this work.},
	pages = {71},
	number = {4},
	journaltitle = {Journal of Intelligent \& Robotic Systems},
	shortjournal = {J Intell Robot Syst},
	author = {Kalaitzakis, Michail and Cain, Brennan and Carroll, Sabrina and Ambrosi, Anand and Whitehead, Camden and Vitzilaios, Nikolaos},
	urldate = {2024-11-29},
	date = {2021-03-26},
	langid = {english},
	keywords = {Artificial Intelligence, Fiducial markers, Localization, Pose Estimation},
	file = {Full Text PDF:C\:\\Users\\Alessandro\\Zotero\\storage\\TS72LR62\\Kalaitzakis et al. - 2021 - Fiducial Markers for Pose Estimation.pdf:application/pdf},
}

@inproceedings{zakiev_virtual_2020,
	title = {Virtual Experiments on {ArUco} and {AprilTag} Systems Comparison for Fiducial Marker Rotation Resistance under Noisy Sensory Data},
	url = {https://ieeexplore.ieee.org/document/9207701},
	doi = {10.1109/IJCNN48605.2020.9207701},
	abstract = {Modern robotic researches propose various machine vision methods for accomplishing robotic tasks. The recognition quality in these tasks is very important for successful performance. A large number of them use fiducial marker systems as a main element of algorithms. However, only a few researches are comparing standard marker systems. This paper is dedicated to the comparison of {AprilTag} and {ArUco} markers resistance to rotations in the presence of synthetic noise. Experiments were conducted in {ROS}/Gazebo virtual environment in order to provide a fair comparison of marker detection and recognition algorithms while eliminating external environment conditions that influence the algorithms' performance. The presented virtual environments allow collecting a significant amount of data by experiment process automation. Different levels of additive white Gaussian noise were applied to input sensory data in order to simulate the imperfection of real digital cameras. The main contribution of the paper is the systematic comparison of {AprilTag} and {ArUco} markers for rotation resistance in the presence of optical sensor noise.},
	eventtitle = {2020 International Joint Conference on Neural Networks ({IJCNN})},
	pages = {1--6},
	booktitle = {2020 International Joint Conference on Neural Networks ({IJCNN})},
	author = {Zakiev, Aufar and Tsoy, Tatyana and Shabalina, Ksenia and Magid, Evgeni and Saha, Subir Kumar},
	urldate = {2024-11-29},
	date = {2020-07},
	note = {{ISSN}: 2161-4407},
	keywords = {Cameras, experimental comparison, fiducial marker system, Gazebo, Immune system, Noise level, recognition algorithms, Reliability, Robot sensing systems, robotics, {ROS}, Virtual environments},
	file = {Full Text PDF:C\:\\Users\\Alessandro\\Zotero\\storage\\5J4RHGHT\\Zakiev et al. - 2020 - Virtual Experiments on ArUco and AprilTag Systems Comparison for Fiducial Marker Rotation Resistance.pdf:application/pdf},
}

@online{noauthor_opencv_nodate,
	title = {{OpenCV}: Camera Calibration and 3D Reconstruction},
	url = {https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html},
	urldate = {2024-11-29},
	file = {OpenCV\: Camera Calibration and 3D Reconstruction:C\:\\Users\\Alessandro\\Zotero\\storage\\UB6IG7S4\\group__calib3d.html:text/html},
}

@online{noauthor_opencv_nodate-1,
	title = {{OpenCV}: Real Time pose estimation of a textured object},
	url = {https://docs.opencv.org/4.x/dc/d2c/tutorial_real_time_pose.html},
	urldate = {2024-11-29},
	file = {OpenCV\: Real Time pose estimation of a textured object:C\:\\Users\\Alessandro\\Zotero\\storage\\DAJS8KWT\\tutorial_real_time_pose.html:text/html},
}

@online{noauthor_designing_2020,
	title = {Designing the perfect Apriltag},
	url = {https://optitag.io/blogs/news/designing-your-perfect-apriltag},
	abstract = {Want to use Apriltags but you're not sure what tag is best for your application. We've got you covered.},
	titleaddon = {Optitag},
	urldate = {2024-11-29},
	date = {2020-06-29},
	langid = {english},
	file = {Snapshot:C\:\\Users\\Alessandro\\Zotero\\storage\\IMNZYTBX\\designing-your-perfect-apriltag.html:text/html},
}

@inreference{noauthor_rodrigues_2024,
	title = {Rodrigues' rotation formula},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Rodrigues%27_rotation_formula&oldid=1250688404},
	abstract = {In the theory of three-dimensional rotation, Rodrigues' rotation formula, named after Olinde Rodrigues, is an efficient algorithm for rotating a vector in space, given an axis and angle of rotation. By extension, this can be used to transform all three basis vectors to compute a rotation matrix in {SO}(3), the group of all rotation matrices, from an axis–angle representation. In terms of Lie theory, the Rodrigues' formula provides an algorithm to compute the exponential map from the Lie algebra so(3) to its Lie group {SO}(3).
This formula is variously credited to Leonhard Euler, Olinde Rodrigues, or a combination of the two. A detailed historical analysis in 1989 concluded that the formula should be attributed to Euler, and recommended calling it "Euler's finite rotation formula." This proposal has received notable support, but some others have viewed the formula as just one of many variations of the Euler–Rodrigues formula, thereby crediting both.},
	booktitle = {Wikipedia},
	urldate = {2024-11-29},
	date = {2024-10-11},
	langid = {english},
	note = {Page Version {ID}: 1250688404},
	file = {Snapshot:C\:\\Users\\Alessandro\\Zotero\\storage\\PD6NSMVL\\Rodrigues'_rotation_formula.html:text/html},
}

@online{noauthor_opencv_nodate-2,
	title = {{OpenCV}: Camera Calibration},
	url = {https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html},
	urldate = {2024-11-29},
	file = {OpenCV\: Camera Calibration:C\:\\Users\\Alessandro\\Zotero\\storage\\T9B3SG8I\\tutorial_py_calibration.html:text/html},
}

@inproceedings{linder_accurate_2020,
	title = {Accurate detection and 3D localization of humans using a novel {YOLO}-based {RGB}-D fusion approach and synthetic training data},
	url = {https://ieeexplore.ieee.org/document/9196899},
	doi = {10.1109/ICRA40945.2020.9196899},
	abstract = {While 2D object detection has made significant progress, robustly localizing objects in 3D space under presence of occlusion is still an unresolved issue. Our focus in this work is on real-time detection of human 3D centroids in {RGB}-D data. We propose an image-based detection approach which extends the {YOLO} v3 architecture with a 3D centroid loss and mid-level feature fusion to exploit complementary information from both modalities. We employ a transfer learning scheme which can benefit from existing large-scale 2D object detection datasets, while at the same time learning end-to-end 3D localization from our highly randomized, diverse synthetic {RGB}-D dataset with precise 3D groundtruth. We further propose a geometrically more accurate depth-aware crop augmentation for training on {RGB}-D data, which helps to improve 3D localization accuracy. In experiments on our challenging intralogistics dataset, we achieve state-of-the-art performance even when learning 3D localization just from synthetic data.},
	eventtitle = {2020 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	pages = {1000--1006},
	booktitle = {2020 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	author = {Linder, Timm and Pfeiffer, Kilian Y. and Vaskevicius, Narunas and Schirmer, Robert and Arras, Kai O.},
	urldate = {2024-12-18},
	date = {2020-05},
	note = {{ISSN}: 2577-087X},
	keywords = {Detectors, Feature extraction, Robustness, Solid modeling, Three-dimensional displays, Training, Two dimensional displays},
	file = {Full Text PDF:C\:\\Users\\Alessandro\\Zotero\\storage\\Y8PM86EB\\Linder et al. - 2020 - Accurate detection and 3D localization of humans using a novel YOLO-based RGB-D fusion approach and.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Alessandro\\Zotero\\storage\\PVJ45Q94\\9196899.html:text/html},
}

@inproceedings{zhang_eye_2019,
	title = {Eye in the Sky: Drone-Based Object Tracking and 3D Localization},
	url = {http://arxiv.org/abs/1910.08259},
	doi = {10.1145/3343031.3350933},
	shorttitle = {Eye in the Sky},
	abstract = {Drones, or general {UAVs}, equipped with a single camera have been widely deployed to a broad range of applications, such as aerial photography, fast goods delivery and most importantly, surveillance. Despite the great progress achieved in computer vision algorithms, these algorithms are not usually optimized for dealing with images or video sequences acquired by drones, due to various challenges such as occlusion, fast camera motion and pose variation. In this paper, a drone-based multi-object tracking and 3D localization scheme is proposed based on the deep learning based object detection. We first combine a multi-object tracking method called {TrackletNet} Tracker ({TNT}) which utilizes temporal and appearance information to track detected objects located on the ground for {UAV} applications. Then, we are also able to localize the tracked ground objects based on the group plane estimated from the Multi-View Stereo technique. The system deployed on the drone can not only detect and track the objects in a scene, but can also localize their 3D coordinates in meters with respect to the drone camera. The experiments have proved our tracker can reliably handle most of the detected objects captured by drones and achieve favorable 3D localization performance when compared with the state-of-the-art methods.},
	pages = {899--907},
	booktitle = {Proceedings of the 27th {ACM} International Conference on Multimedia},
	author = {Zhang, Haotian and Wang, Gaoang and Lei, Zhichao and Hwang, Jenq-Neng},
	urldate = {2024-12-18},
	date = {2019-10-15},
	eprinttype = {arxiv},
	eprint = {1910.08259 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {Full Text PDF:C\:\\Users\\Alessandro\\Zotero\\storage\\I9XV2FHR\\Zhang et al. - 2019 - Eye in the Sky Drone-Based Object Tracking and 3D Localization.pdf:application/pdf;Snapshot:C\:\\Users\\Alessandro\\Zotero\\storage\\AYJYMFK4\\1910.html:text/html},
}

@article{yong_object_2023,
	title = {Object Detection and Distance Measurement Algorithm for Collision Avoidance of Precast Concrete Installation during Crane Lifting Process},
	volume = {13},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2075-5309},
	url = {https://www.mdpi.com/2075-5309/13/10/2551},
	doi = {10.3390/buildings13102551},
	abstract = {In the construction industry, the process of carrying heavy loads from one location to another by means of a crane is inevitable. This reliance on cranes to carry heavy loads is more obvious when it comes to high-rise building construction. Depending on the conditions and requirements on-site, various types of construction lifting equipment (i.e., cranes) are being used. As off-site construction ({OSC}) is gaining more traction recently, cranes are becoming more important throughout the construction project as precast concrete ({PC}) members are major components of {OSC} calling for lifting work. As a result of the increased use of cranes on construction sites, concerns about construction safety as well as the effectiveness of existing load collision prevention systems are attracting more attention from various parties involved. Besides the inherent risks associated with heavy load lifting, the unpredictable movement of on-site workers around the crane operation area, along with the presence of blind spots that obstruct the crane operator’s field-of-view ({FOV}), further increase the accident probability during crane operation. As such, the need for a more reliable and improved collision avoidance system that prevents lifted loads from hitting other structures and workers is paramount. This study introduces the application of deep learning-based object detection and distance measurement sensors integrated in a complementary way to achieve the stated need. Specifically, the object detection technique was used with the application of an Internet Protocol ({IP}) camera to detect the workers within the crane operation radius, whereas ultrasonic sensors were used to measure the distance of surrounding obstacles. Both applications were designed to work concurrently so as to prevent potential collisions during crane lifting operations. The field testing and evaluation of the integrated system showed promising results.},
	pages = {2551},
	number = {10},
	journaltitle = {Buildings},
	author = {Yong, Yik Pong and Lee, Seo Joon and Chang, Young Hee and Lee, Kyu Hyup and Kwon, Soon Wook and Cho, Chung Suk and Chung, Su Wan},
	urldate = {2024-12-18},
	date = {2023-10},
	langid = {english},
	note = {Number: 10
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {collision avoidance, computer vision, crane, distance measurement sensor, object detection, off-site construction, precast concrete},
	file = {Full Text PDF:C\:\\Users\\Alessandro\\Zotero\\storage\\SSWI98PQ\\Yong et al. - 2023 - Object Detection and Distance Measurement Algorithm for Collision Avoidance of Precast Concrete Inst.pdf:application/pdf},
}

@article{zhou_image-based_2021,
	title = {Image-based onsite object recognition for automatic crane lifting tasks},
	volume = {123},
	issn = {0926-5805},
	url = {https://www.sciencedirect.com/science/article/pii/S0926580520311079},
	doi = {10.1016/j.autcon.2020.103527},
	abstract = {The construction industry is suffering from aging workers and frequent accidents, as well as low productivity. Automation and robotics is regarded as a promising approach for enhancing the development of the industry, and the automatic operation of cranes, as an important aspect of construction, is attracting increasing attention. However, due to the complexity and dynamics of construction sites, it is difficult for cranes to automatically recognize and locate lifting objects (e.g., precast facades and partitions) on site. To solve this problem, an image-based automated onsite object recognition approach for the automatic operation of cranes is developed in this study. This is a fusion of Faster-R-{CNN} (Region-based Convolutional Neural Network), Canny, Hough Transformation, Endpoint clustering analysis and Vertex-based Determining Model, to uniquely locate a lifting object with exact pose and extract its features (e.g., centroid coordinates, size, and color). Based on the extracted features, the lifting object can be retrieved in the database with the {IFC} (Industry Foundation Classes) format of {BIM} (Building Information Modeling) to obtain more features for the automatic operation of a crane. It is shown from a field experiment that the developed approach is workable and has the potential to support the automatic operation of cranes. This contributes a basic approach to the automatic operation of cranes and promotes the rapid development of construction automation and robotics.},
	pages = {103527},
	journaltitle = {Automation in Construction},
	shortjournal = {Automation in Construction},
	author = {Zhou, Ying and Guo, Hongling and Ma, Ling and Zhang, Zhitian and Skitmore, Martin},
	urldate = {2024-12-18},
	date = {2021-03-01},
	keywords = {Automated object recognition, Automatic operation of cranes, Image processing, Vertex-based determining model},
	file = {ScienceDirect Snapshot:C\:\\Users\\Alessandro\\Zotero\\storage\\JGQMVDMC\\S0926580520311079.html:text/html;Volltext:C\:\\Users\\Alessandro\\Zotero\\storage\\H4HWPY57\\Zhou et al. - 2021 - Image-based onsite object recognition for automatic crane lifting tasks.pdf:application/pdf},
}
