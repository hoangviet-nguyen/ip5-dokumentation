\section{Implementierung}

In diesem Kapitel wird die Implementierung erläutert vom Marker Konzept. 
Es werden die Umsetzung der Intrinsischen Kamera Kalibrierung beschrieben und die zwei Implementierungen, eine mit ArUco Marker der andere mit AprilTags, erläutert und evaluiert.
Zusätzlich werden die Umsetzungen der Posenschätzung und der Mittelpunkt-Berechnung beschrieben.

\subsection{Intrinsische Kamera Kalibrierung}

Um eine genaue Posenschätzung durchführen zu können, braucht man erst die Kamera Matrix und die Verzerrungs-Koeffizienten. 
Die Kamera Matrix ist eine 3x3 Matrix, welches die Fokus Länge und optische Zentren besitzt. 
Die Kamera Matrix würde so aussehen:
\begin{bmatrix}
fx & 0 & cx \\ 
0 & fy & cy \\ 
0 & 0  & 1 
\end{bmatrix}

Um eine Kalibrirung durchzuführen, braucht es Test-Fotos eines Kalibrierungs-Muster. 
Die Abbildung\ref{fig:Pattern} zeigt ein 9x6 Schachbrett-Muster, welches für diese Implementierung benutzt worden ist, um die Kalibrierung durchzuführen.
Für genauere Kalibrierungs-Werte sollten mindestens 10 Fotos vom Schachbrett-Muster gemacht werden\cite{noauthor_opencv_nodate-2}


\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{graphics/pattern.png}
    \caption{Ein 9x6 Schachbrett-Muster, welches für Kamera-Kalibrierung benutzt wird}
    \label{fig:Pattern}
\end{figure}

\begin{lstlisting}[language=Python, caption=Bilder laden für Kalibrierung]
    image_files = glob.glob('./imgs/*.jpg')
    images = [cv2.imread(file) for file in image_files]
\end{lstlisting}

Alle Bilder innerhalb des "./imgs" Ordner werden geladen und als cv image abgespeichert. 

\begin{lstlisting}[language=Python, caption=Methode um Schachbrett Ecken zu finden]
    def find_chessboard_corners(images, pattern_size):
    obj_points = []
    img_points = []

    # Prepare the 3D points of the chessboard corners (0,0,0), (1,0,0), (2,0,0) ..., (6,5,0)
    objp = np.zeros((pattern_size[0] * pattern_size[1], 3), np.float32)
    objp[:, :2] = np.mgrid[0:pattern_size[0], 0:pattern_size[1]].T.reshape(-1, 2)

    for img in images:
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        ret, corners = cv2.findChessboardCorners(gray, pattern_size, None)

        if ret:
            obj_points.append(objp)
            img_points.append(corners)

    return obj_points, img_points
\end{lstlisting}

In der Methode find_chessboard_corners werden die Object Points und image Points ausgegeben. Object Points sind die 3D Punkte der Schachbrett-Muster Ecken.
Dann wird jedes Bild zu einem Graustufen-Bild umgewandelt, da dass SchabrettMuster Schwarz-Weiss ist, braucht es die RGB Werte nicht. 
Durch die findChessboardCorners von opencv erhält man alle image points, also die Ecken des Schachbrett-Musters, und ret. 
Ret besagt ob ein Schabrett-Muster gefunden worden ist und falls das der Fall ist werden die Object Points und Image Points erweitert.

\begin{lstlisting}[language=Python, caption=Methode um Kamera zu kalibrieren]
    def calibrate_camera(obj_points, img_points, img_size):
        ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(obj_points, img_points, img_size, None, None)
        return ret, mtx, dist, rvecs, tvecs
\end{lstlisting}

Die Object Points und Image Points können dann in die calibrateCamera Methode von opencv eingefügt werden, um die Kamera Matrix und die Verzerrungs-Koeffizienten zu bekommen.

\begin{lstlisting}[language=Python, caption=Speicherung der Kalibrierungs-Daten in einer Datei]
    # Save parameters to a file
    cv_file = cv2.FileStorage(save_path, cv2.FILE_STORAGE_WRITE)
    cv_file.write('K', camera_matrix)
    cv_file.write('D', dist_coeffs)
    cv_file.release()
\end{lstlisting}

Zuletzt werden die Kamera Matrix und Verzerrungs Koeffizienten in eine Datei gespeichert, so dass diese bei einem neuen Start wieder verwendet werden können ohne kalibrieren zu müssen.

Falls sich der Fokus der Kamera oder eine andere Kamera Eigenschaft verändert wird, muss diese Kalibrierung neu gemacht werden, um so die neue Kalibrierungs Datei benutzen zu können.

\subsection{Lokalisierung der Last}

\subsubsection{Implementierung: ArUco}

\begin{lstlisting}[language=Python, caption=ID und Ecken eines ArUco Markers erkennen]
    marker_dict = aruco.getPredefinedDictionary(cv.aruco.DICT_4X4_1000)
    param_markers = cv.aruco.DetectorParameters()
    detector = cv.aruco.ArucoDetector(marker_dict, param_markers)
    marker_corners, marker_IDs, reject = detector.detectMarkers(image)
    
    total_markers = range(0, marker_IDs.size)
\end{lstlisting}

Um ein ArUco Marker erkennen zu können muss man erst die Dictionary des Ausgewählten Markers definieren. 
In dieser Implementierung ist es die DICT_4X4_1000.
Danach muss ein Detector initialisiert werden und damit kann man in einem Bild alle Marker Ecken und IDs bekommen.
Durch diese kann man dann durch iterieren um für jeden gefundenen Marker eine Posenschätzung durchzuführen.

\subsubsection{Implementierung: AprilTag}
Um AprilTags erkennen zu können verwenden wir die Github Resource \cite{apriltag_github}.
Die Software wird unter der BSD 2-Clause License veröffentlicht und kann unter bestimmten Bedingungen
Frei genutzt werden. Wir stellen die Benötigten Binär Dateien in unseren Projekt zur verfügung.

Folgendes Code Beispiel zeigt wie der AprilTag im einem Bild erkennt werden kann.

\lstset{
  language=Python,
  basicstyle=\ttfamily\small,
  keywordstyle=\bfseries\color{blue},
  commentstyle=\itshape\color{green!50!black},
  stringstyle=\color{orange},
  showstringspaces=false,
  numbers=left,
  numberstyle=\tiny,
  stepnumber=1,
  numbersep=5pt
}

\begin{lstlisting}
from apriltag import apriltag
from cv2 import undistort

img = "path/to/image"
undistorted = undistort(img, calibration_matrix, distortion)
detector = apriltag("tagStandard41h12")
detected_marker = detector.detect(undistorted)
\end{lstlisting}


Die Variable \texttt{detected\_marker} enthält eine Liste der erkannten Marker, einschließlich ihrer Tag-Nummern.
Zusätzlich liefert die Variable die Koordinaten der Eckpunkte der Marker im Bild sowie 
die Koordinaten des Zentrums jedes Markers. Diese Informationen sind essenziell, um die Marker 
präzise zu lokalisieren und weitere Berechnungen, wie die Positionsbestimmung, durchzuführen.



\subsubsection{Umsetzung Posenschätzung}
Die Posenschätzung ermöglicht es, die Position und Orientierung eines Markers relativ zur Kamera
zu bestimmen. Dafür werden die intrinsischen Kameraparameter, die Eckpunkte des erkannten 
Markers (in Bildkoordinaten) sowie die reale Größe des Markers benötigt. Die Größe sollte 
in einer einheitlichen Maßeinheit angegeben werden, in unserem Fall Zentimeter (cm).

Der folgende Code implementiert die Posenschätzung für einen einzelnen Marker:

\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\bfseries\color{blue},
    commentstyle=\itshape\color{green!50!black},
    stringstyle=\color{orange},
    showstringspaces=false,
    numbers=left,
    numberstyle=\tiny,
    stepnumber=1,
    numbersep=4pt
}


\begin{lstlisting}
def estimatePoseSingleMarkers(corners, marker_size, mtx, distortion):
    '''
    This will estimate the rvec and tvec for each of the marker corners 
    detected by:

    corners - array of detected corners for each detected marker
    marker_size - is the size of the detected markers
    mtx - is the camera matrix
    distortion - is the camera distortion matrix
    RETURN list of rvecs, tvecs
    '''
    points = np.array([[-marker_size / 2, marker_size / 2, 0],
                              [marker_size / 2, marker_size / 2, 0],
                              [marker_size / 2, -marker_size / 2, 0],
                              [-marker_size / 2, -marker_size / 2, 0]])

    _, R, t = cv2.solvePnP(
        marker_points, 
        corners, mtx, 
        distortion,
        cv2.SOLVEPNP_IPPE_SQUARE
    )
    
    return np.array([R]).flatten(), np.array([t]).flatten()
\end{lstlisting}

Die Variable \texttt{points} enthält die 3D-Koordinaten der Ecken des Markers in der realen Welt. 
Diese werden für die Berechnungen der Methode cv2.SOLVEPNP\_IPPE\_SQUARE benötigt. 
Die Methode erfordert, dass der Marker als ein Quadrat definiert wird, dessen Seiten parallel 
zu den Achsen des Koordinatensystems liegen.

Die Funktion gibt zwei Vektoren zurück:
\begin{itemize}
    \item \textbf{Rotationsvektor} (\texttt{rvec}): Beschreibt die Orientierung des Markers relativ zur Kamera.
    \item \textbf{Translationsvektor} (\texttt{tvec}): Gibt die Position des Markers relativ zur Kamera an.
\end{itemize}

Diese Ergebnisse können verwendet werden, um die Traverse korrekt zu positionieren oder andere 
räumliche Transformationen durchzuführen.


\subsubsection{Umsetzung der Mittelpunkt-Berechnung}

Da man nicht die Koordinaten der Marker braucht, sondern die des Anschlagspunkts, muss man eine Mittelpunkt-Berechnung durchführen wie im Kapitel \ref{sec:middlePoint} beschrieben.

\begin{lstlisting}[language=Python, caption=ID und Ecken eines ArUco Markers erkennen]
    pointTranslations = [np.array([6.3,0,0]),np.array([0,-6.3,0]),np.array([-6.3,0,0]),np.array([0,6.3,0])]

    rVec, tVec, _ = my_estimatePoseSingleMarkers(corners, markerSize, mtx, dist)
    tVec = np.array(tVec)
    rVec = np.array(rVec)

    rmtx,_ = cv2.Rodrigues(rVec[i][0])
    translate = np.matmul(rmtx, pointTranslations[ids%4])
    point = np.add(tVec[i][0].flatten(),translate)
\end{lstlisting}

Man führt zuerst die Posenschätzung aus um die Translations und Rotations Vektoren zu bekommen.
Mit der Rodrigues Methode kann man den Rotations Vektor zu einer Rotations Matrix umwandeln.
Dann muss man die Translation zur Mitte zusammen mit der Rotationsmatrix zusammenrechnen um so eine richtig rotierte Translation zur Mitte zu bekommen.
Welche Translation benutzt werden muss wird, wie im Kapitel \ref{sec:middlePoint} beschrieben, durch die ID des Markers Modulo 4 definiert, da es 4 pro Anschlagspunkt gibt.

Schlussendlich wird auf die Translations Vektor vom Marker die Tranlation zur Mitte addiert und so bekommt man den Mittelpunkt der Anordnung.




