\section{Implementierung}

In diesem Kapitel wird die Implementierung erläutert vom Marker Konzept. 
Es werden die Umsetzung der Intrinsischen Kamera Kalibrierung beschrieben und die zwei Implementierungen, eine mit ArUco Marker der andere mit AprilTags, erläutert und evaluiert.
Zusätzlich werden die Umsetzungen der Posenschätzung und der Mittelpunkt-Berechnung beschrieben.

\subsection{Intrinsische Kamera Kalibrierung}

Um eine genaue Posenschätzung durchführen zu können, braucht man erst die Kamera Matrix und die Verzerrungs-Koeffizienten. 
Die Kamera Matrix ist eine 3x3 Matrix, welches die Fokus Länge und optische Zentren besitzt. 
Die Kamera Matrix würde so aussehen:
\begin{bmatrix}
fx & 0 & cx \\ 
0 & fy & cy \\ 
0 & 0  & 1 
\end{bmatrix}

Um eine Kalibrirung durchzuführen, braucht es Test-Fotos eines Kalibrierungs-Muster. 
Die Abbildung\ref{fig:Pattern} zeigt ein 9x6 Schachbrett-Muster, welches für diese Implementierung benutzt worden ist, um die Kalibrierung durchzuführen.
Für genauere Kalibrierungs-Werte sollten mindestens 10 Fotos vom Schachbrett-Muster gemacht werden\cite{noauthor_opencv_nodate-2}


\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{graphics/pattern.png}
    \caption{Ein 9x6 Schachbrett-Muster, welches für Kamera-Kalibrierung benutzt wird}
    \label{fig:Pattern}
\end{figure}

\begin{lstlisting}[language=Python, caption=Bilder laden für Kalibrierung]
    image_files = glob.glob('./imgs/*.jpg')
    images = [cv2.imread(file) for file in image_files]
\end{lstlisting}

Alle Bilder innerhalb des "./imgs" Ordner werden geladen und als cv image abgespeichert. 

\begin{lstlisting}[language=Python, caption=Methode um Schachbrett Ecken zu finden]
    def find_chessboard_corners(images, pattern_size):
    obj_points = []
    img_points = []

    # Prepare the 3D points of the chessboard corners (0,0,0), (1,0,0), (2,0,0) ..., (6,5,0)
    objp = np.zeros((pattern_size[0] * pattern_size[1], 3), np.float32)
    objp[:, :2] = np.mgrid[0:pattern_size[0], 0:pattern_size[1]].T.reshape(-1, 2)

    for img in images:
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        ret, corners = cv2.findChessboardCorners(gray, pattern_size, None)

        if ret:
            obj_points.append(objp)
            img_points.append(corners)

    return obj_points, img_points
\end{lstlisting}

In der Methode find_chessboard_corners werden die Object Points und image Points ausgegeben. Object Points sind die 3D Punkte der Schachbrett-Muster Ecken.
Dann wird jedes Bild zu einem Graustufen-Bild umgewandelt, da dass SchabrettMuster Schwarz-Weiss ist, braucht es die RGB Werte nicht. 
Durch die findChessboardCorners von opencv erhält man alle image points, also die Ecken des Schachbrett-Musters, und ret. 
Ret besagt ob ein Schabrett-Muster gefunden worden ist und falls das der Fall ist werden die Object Points und Image Points erweitert.

\begin{lstlisting}[language=Python, caption=Methode um Kamera zu kalibrieren]
    def calibrate_camera(obj_points, img_points, img_size):
        ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(obj_points, img_points, img_size, None, None)
        return ret, mtx, dist, rvecs, tvecs
\end{lstlisting}

Die Object Points und Image Points können dann in die calibrateCamera Methode von opencv eingefügt werden, um die Kamera Matrix und die Verzerrungs-Koeffizienten zu bekommen.

\begin{lstlisting}[language=Python, caption=Speicherung der Kalibrierungs-Daten in einer Datei]
    # Save parameters to a file
    cv_file = cv2.FileStorage(save_path, cv2.FILE_STORAGE_WRITE)
    cv_file.write('K', camera_matrix)
    cv_file.write('D', dist_coeffs)
    cv_file.release()
\end{lstlisting}

Zuletzt werden die Kamera Matrix und Verzerrungs Koeffizienten in eine Datei gespeichert, so dass diese bei einem neuen Start wieder verwendet werden können ohne kalibrieren zu müssen.

Falls sich der Fokus der Kamera oder eine andere Kamera Eigenschaft verändert wird, muss diese Kalibrierung neu gemacht werden, um so die neue Kalibrierungs Datei benutzen zu können.

\subsection{Lokalisierung der Last}

\subsubsection{Implementierung: ArUco}

\begin{lstlisting}[language=Python, caption=ID und Ecken eines ArUco Markers erkennen]
    marker_dict = aruco.getPredefinedDictionary(cv.aruco.DICT_4X4_1000)
    param_markers = cv.aruco.DetectorParameters()
    detector = cv.aruco.ArucoDetector(marker_dict, param_markers)
    marker_corners, marker_IDs, reject = detector.detectMarkers(image)
    
    total_markers = range(0, marker_IDs.size)
\end{lstlisting}

Um ein ArUco Marker erkennen zu können muss man erst die Dictionary des Ausgewählten Markers definieren. 
In dieser Implementierung ist es die DICT_4X4_1000.
Danach muss ein Detector initialisiert werden und damit kann man in einem Bild alle Marker Ecken und IDs bekommen.
Durch diese kann man dann durch iterieren um für jeden gefundenen Marker eine Posenschätzung durchzuführen.

\subsubsection{Implementierung: AprilTag}

\subsubsection{Umsetzung Posenschätzung der Marker}
(Erklärung der Umsetzung der Posenschätzung sowie die ersten Resultate zeigen und evaluieren welche besser ist)

\subsubsection{Umsetzung der Mittelpunkt-Berechnung}

Da man nicht die Koordinaten der Marker braucht, sondern die des Anschlagspunkts, muss man eine Mittelpunkt-Berechnung durchführen wie im Kapitel \ref{sec:middlePoint} beschrieben.

\begin{lstlisting}[language=Python, caption=ID und Ecken eines ArUco Markers erkennen]
    pointTranslations = [np.array([6.3,0,0]),np.array([0,-6.3,0]),np.array([-6.3,0,0]),np.array([0,6.3,0])]

    rVec, tVec, _ = my_estimatePoseSingleMarkers(corners, markerSize, mtx, dist)
    tVec = np.array(tVec)
    rVec = np.array(rVec)

    rmtx,_ = cv2.Rodrigues(rVec[i][0])
    translate = np.matmul(rmtx, pointTranslations[ids%4])
    point = np.add(tVec[i][0].flatten(),translate)
\end{lstlisting}

Man führt zuerst die Posenschätzung aus um die Translations und Rotations Vektoren zu bekommen.
Mit der Rodrigues Methode kann man den Rotations Vektor zu einer Rotations Matrix umwandeln.
Dann muss man die Translation zur Mitte zusammen mit der Rotationsmatrix zusammenrechnen um so eine richtig rotierte Translation zur Mitte zu bekommen.
Welche Translation benutzt werden muss wird, wie im Kapitel \ref{sec:middlePoint} beschrieben, durch die ID des Markers Modulo 4 definiert, da es 4 pro Anschlagspunkt gibt.

Schlussendlich wird auf die Translations Vektor vom Marker die Tranlation zur Mitte addiert und so bekommt man den Mittelpunkt der Anordnung.



